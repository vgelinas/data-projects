{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitbit Data Visualisation in Python\n",
    "(See associated [blog posting](https://vgelinas.github.io/post/fitbit-data-exploration-part-i/)).\n",
    "\n",
    "In this project we will explore some Fitbit activity data pulled via [orcasgit's python-fitbit api](https://github.com/orcasgit/python-fitbit). We will go through the following steps:\n",
    "1. Data collection\n",
    "2. Data cleaning\n",
    "3. Data visualisation\n",
    "\n",
    "### Dependencies\n",
    "* Python 3+\n",
    "* The [python-fitbit api](https://pypi.org/project/fitbit/)\n",
    "* The [ratelimit package](https://pypi.org/project/ratelimit/)\n",
    "* The datetime, json, matplotlib and pandas standard libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitbit\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "We do this in two steps:\n",
    "* We first access the API via python-fitbit, dealing with the necessary authentication steps.\n",
    "* We then sample some responses, and build datasets by querying over a range of dates.\n",
    "\n",
    "### 1.1. Authentication setup\n",
    "\n",
    "To collect personal data, we first need to [set-up a Fitbit app](https://dev.fitbit.com/apps/new), and to collect the client_id and client_secret for this app. For this project I've chosen to keep these in a credentials.json file stored in a dedicated subfolder named 'oauth', but just make sure you have these on hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat oauth/credentials.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need tokens for authentication. We need:\n",
    "\n",
    "* An access token.\n",
    "* A refresh token.\n",
    "* An expiration time for the access token (the refresh token never expires).\n",
    "\n",
    "These can be obtained by going to the [Manage my apps](https://dev.fitbit.com/apps) section on the Fitbit website, selecting your app and navigating to \"OAuth 2.0 tutorial page\". Alternatively, you can run the script \"gather_keys_oauth2.py\" from the python-fitbit [github page](https://github.com/orcasgit/python-fitbit), in which case you should set your Fitbit app's callback URL to https://127.0.0.1:8080/. \n",
    "\n",
    "The access token serves to authenticate and typically expires after ~8 hours. The refresh token is then used to obtain a new pair (access_token, refresh_token) from the API. Similar to above, I chose to store these in a json file named 'tokens'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat oauth/tokens.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only important keys above are \"access_token\", \"refresh_token\" and \"expires_at\" (the rest corresponds to optional arguments). \n",
    "\n",
    "Next up, the code below instantiates a fitbit client which will handle API calls for us. We pass along the credentials and tokens as arguments, and we also pass a \"token refresh\" function which will store the new (access_token, refresh_token) pair sent by the API whenever the first one expires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "with open(\"./oauth/credentials.json\", \"r\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "# Load tokens\n",
    "with open(\"./oauth/tokens.json\", \"r\") as f:  \n",
    "    tokens = json.load(f)  \n",
    "\n",
    "client_id = credentials['client_id'] \n",
    "client_secret = credentials['client_secret']\n",
    "access_token = tokens['access_token']\n",
    "refresh_token = tokens['refresh_token']\n",
    "expires_at = tokens['expires_at'] \n",
    "\n",
    "# Token refresh method \n",
    "def refresh_callback(token):   \n",
    "    \"\"\" Called when the OAuth token has been refreshed \"\"\" \n",
    "    with open(\"./oauth/tokens.json\", \"w\") as f: \n",
    "        json.dump(token, f)  \n",
    "\n",
    "# Initialise client  \n",
    "client = fitbit.Fitbit(client_id=client_id, \n",
    "                       client_secret=client_secret,\n",
    "                       access_token=access_token,\n",
    "                       refresh_token=refresh_token,\n",
    "                       refresh_cb=refresh_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time this is called you should be served an authorisation page for authentication, but afterwards the refresh token song & dance should handle this in the background, and we won't need to set it up again unless you lose your tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. A first look at the response data\n",
    "The python-fitbit api supports the methods listed [here](https://python-fitbit.readthedocs.io/en/latest/#fitbit-api). For example, we could call:\n",
    "\n",
    "* **client.sleep**, to get basic sleep data (bed time and wake time, time awake at night, ...).\n",
    "* **client.activities**, to get timestamps for activities (walking, running, cycling, ...) and summary data (number of steps, minutes active, ...).\n",
    "* **client.intraday_time_series**, to get granular data on various activities (such as heart rate or steps rate for every minute of the day).\n",
    "\n",
    "We'll be interested in the activities and intraday steps data. Now, let's take a look at the response for one date, say May 1st."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activity data for May 1st\n",
    "# The API takes a date formatted as 'YYYY-MM-DD'\n",
    "date = '2020-05-01'\n",
    "activities_response = client.activities(date=date)\n",
    "\n",
    "# Display response\n",
    "activities_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the type of the response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(activities_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response consists of nested dictionaries. We'll extract two datasets from the 'activities' and 'summary' keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activities dataset\n",
    "activities = activities_response['activities']\n",
    "activities = pd.DataFrame(activities)\n",
    "activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary dataset\n",
    "summary = activities_response['summary']\n",
    "\n",
    "# Remove sub-dictionaries\n",
    "del summary['distances']\n",
    "del summary['heartRateZones']\n",
    "\n",
    "summary = pd.DataFrame(summary, index=[0])  # all values are scalars, must pass an index\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the intraday step data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intraday steps data\n",
    "steps_response = client.intraday_time_series('activities/steps', base_date=date, detail_level=\"1min\")\n",
    "\n",
    "# Extract dataset from response object\n",
    "steps = steps_response['activities-steps-intraday']['dataset']\n",
    "\n",
    "# Display dataset\n",
    "steps = pd.DataFrame(steps)\n",
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the minute-by-minute count of steps on that day. Let's take a quick look at a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Collect activity and intraday steps data since October 1st.\n",
    "\n",
    "We can now build our datasets, which will consists of general activity data and intraday steps data from October 1st to yesterday. We will:\n",
    "\n",
    "* Produce a list of dates in 'YYYY-MM-DD' string format for our queries.\n",
    "* Query the API for each date, extracting our 'activities', 'summary' and 'steps' datasets from the response.\n",
    "* Limit our query rate to 150/hour (since this is the Fitbit API rate limit).\n",
    "* Combine and store the results.\n",
    "\n",
    "First, let's get a list of dates. We can use the pandas **date_range** method to produce a list of datetime objects, and format them using the **strftime** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get date range from October 1st to yesterday\n",
    "start = pd.to_datetime(\"2019-10-01\")\n",
    "date_range = pd.date_range(start=start, end=datetime.today() - timedelta(days=1))\n",
    "date_range = [datetime.strftime(date, \"%Y-%m-%d\") for date in date_range]\n",
    "date_range[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we query the API for each date in date_range. \n",
    "\n",
    "As seen when we first took a look at the response data, we actually make two API calls per date (i.e. client.activities and client.intraday_time_series). Since the Fitbit API has a rate limit of 150 calls/hour, we should query at most 75 dates an hour. We can accomplish this via the [ratelimit](https://pypi.org/project/ratelimit/) package, which lets you limit the number of times a function is called over a time period.\n",
    "\n",
    "Finally, we call the API for each day, timestamp the resulting datasets, and store the total in csv files locally.\n",
    "We do this for each of the 'activities', 'summary' and 'steps' datasets. The script below accomplishes this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a data collection function, and we use the ratelimit package\n",
    "# to limit our function to 150 API calls / hour.\n",
    "ONE_HOUR = 3600\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=70, period=ONE_HOUR)\n",
    "def call_fitbit_api(date):\n",
    "    \"\"\" Call the Fitbit API for given date in format 'YYYY-MM-DD',\n",
    "        Return tuple (activities, summary, steps) of dataframes \"\"\"\n",
    "    \n",
    "    # Call API twice to get activities and steps responses\n",
    "    activities_data = client.activities(date=date)\n",
    "    steps_data = client.intraday_time_series('activities/steps', base_date=date, detail_level='1min')\n",
    "        \n",
    "    # Get activities dataset\n",
    "    activities = activities_data['activities']\n",
    "    activities = pd.DataFrame(activities)\n",
    "    \n",
    "    # Get summary dataset\n",
    "    summary = activities_data['summary']\n",
    "    del summary['distances']\n",
    "    del summary['heartRateZones']\n",
    "    summary = pd.DataFrame(summary, index=[0])\n",
    "        \n",
    "    # Get steps intraday dataset  \n",
    "    steps = steps_data['activities-steps-intraday']['dataset']\n",
    "    steps = pd.DataFrame(steps)\n",
    "    \n",
    "    # Add a date column\n",
    "    activities['date'] = [date for i in activities.index]\n",
    "    summary['date'] = [date]\n",
    "    steps['date'] = [date for i in steps.index]\n",
    "    \n",
    "    return activities, summary, steps\n",
    "\n",
    "\n",
    "def get_fitbit_data(date_range):\n",
    "    \"\"\" Collect 'activities', 'summary' and 'steps' datasets over given dates\n",
    "        Store as CSV files with format RESOURCE_DATE_to_DATE.csv \"\"\"\n",
    "    \n",
    "    daily_df = {\n",
    "        'activities': [],\n",
    "        'summary': [],\n",
    "        'steps': []\n",
    "    }\n",
    "\n",
    "    for date in date_range:\n",
    "        # Call API and get three datasets\n",
    "        activities, summary, steps = call_fitbit_api(date)\n",
    "    \n",
    "        # Append to previous datasets\n",
    "        daily_df['activities'].append(activities)\n",
    "        daily_df['summary'].append(summary)\n",
    "        daily_df['steps'].append(steps)\n",
    "        \n",
    "    # Store total dataset as file with format \"resource_DATE_to_DATE.csv\"\n",
    "    start, end = date_range[0], date_range[-1]\n",
    "\n",
    "    for resource in daily_df:\n",
    "        df = pd.concat(daily_df[resource], ignore_index=True)\n",
    "        df.to_csv(\"./data/raw/{}_{}_to_{}.csv\".format(resource, start, end), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Fitbit 'activities', 'summary' and 'steps' data since October 1st, 2019\n",
    "get_fitbit_data(date_range=date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the data\n",
    "\n",
    "It's time to take a look at each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. The activity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = pd.read_csv(\"./data/raw/activities_2019-10-01_to_2020-05-18.csv\")\n",
    "activities.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 16 columns, many of which contain logging information, True/False data or duplicate information which is not useful to us. Let's drop these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['activityId', 'activityParentId', 'activityParentName', 'hasStartTime', \n",
    "                'isFavorite', 'lastModified', 'logId', 'startDate']\n",
    "\n",
    "activities.drop(drop_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the distance column. Consulting the documentation, we see that this means logged distance. Since I've rarely used the feature, it looks like the column consists mostly of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities.distance.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only have 2 non-missing values in 354 rows, let's drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities.drop('distance', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the column names are in camelCase. Let's rename them to Python's favored snake_case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities.rename(columns={'startTime': 'start_time'}, inplace=True)\n",
    "activities.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duration column isn't easy to parse and is missing units. The Fitbit api [documentation](https://dev.fitbit.com/build/reference/web-api/activity/#activity-logging) lists the duration as being in millisecond, so let's put it in minutes and rename accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities.duration = activities.duration.apply(lambda x: round(x/60000))\n",
    "activities.rename(columns={'duration': 'duration_min'}, inplace=True)\n",
    "\n",
    "activities.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with analysis, let's format the start_time column as \"YYYY-MM-DD H:M:S\" to more easily convert to a datetime object. Since we have the activity duration, we can also add an end_time column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format start_time column and convert to datetime object\n",
    "activities.start_time = activities.date + \" \" + activities.start_time + \":00\"\n",
    "activities.start_time = pd.to_datetime(activities.start_time)\n",
    "\n",
    "# Create end_time column by adding the duration_min column to start_time\n",
    "activities_duration = activities.duration_min.apply(lambda x: timedelta(minutes=x))\n",
    "activities['end_time'] = activities.start_time + activities_duration\n",
    "\n",
    "# Display result\n",
    "activities.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's reorder the columns for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "column_order = ['date', 'name', 'description', 'start_time', 'end_time', 'duration_min', 'steps', 'calories']\n",
    "activities = activities[column_order]\n",
    "\n",
    "# Store dataset\n",
    "start, end = date_range[0], date_range[-1]\n",
    "activities.to_csv(\"./data/tidy/activities_{}_to_{}.csv\".format(start, end), index=False)\n",
    "\n",
    "# Look at end result\n",
    "activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The summary dataset\n",
    "\n",
    "Now let's take a look at the second dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.read_csv(\"./data/raw/summary_2019-10-01_to_2020-05-18.csv\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the activeScore column is added by the python-fitbit wrapper to the Fitbit API. All values are -1 in our dataset so there's not much loss of information in dropping the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(summary.activeScore == -1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.drop('activeScore', axis=1, inplace=True)\n",
    "summary.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we again format all columns to snake_case and reorder for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to snake_case\n",
    "columns_map = {\n",
    "    'activityCalories': 'activity_calories',\n",
    "    'caloriesBMR': 'calories_BMR',\n",
    "    'caloriesOut': 'calories_out',\n",
    "    'fairlyActiveMinutes': 'fairly_active_minutes',\n",
    "    'lightlyActiveMinutes': 'lightly_active_minutes',\n",
    "    'marginalCalories': 'marginal_calories',\n",
    "    'restingHeartRate': 'resting_heart_rate',\n",
    "    'sedentaryMinutes': 'sedentary_minutes',\n",
    "    'veryActiveMinutes': 'very_active_minutes'\n",
    "}\n",
    "\n",
    "summary.rename(columns=columns_map, inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "column_order = ['date', 'steps', 'very_active_minutes', 'fairly_active_minutes', 'lightly_active_minutes', \n",
    "                'sedentary_minutes', 'activity_calories', 'marginal_calories', 'calories_out', 'calories_BMR',\n",
    "                'resting_heart_rate']\n",
    "\n",
    "summary = summary[column_order]\n",
    "\n",
    "# Store dataset\n",
    "start, end = summary.date[0], summary.date[len(summary.index)-1]\n",
    "summary.to_csv(\"./data/tidy/summary_{}_to_{}.csv\".format(start, end), index=False)\n",
    "\n",
    "# Look at result\n",
    "summary.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. The steps dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at the intraday steps dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = pd.read_csv(\"./data/raw/steps_2019-10-01_to_2020-05-18.csv\")\n",
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the time and date into a single column, in datetime format. We also rename value to the more descriptive 'stepcount'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine date and time\n",
    "steps.time = steps.date + \" \" + steps.time\n",
    "\n",
    "# Rename value to stepcount\n",
    "steps.rename(columns={'value': 'stepcount'}, inplace=True)\n",
    "\n",
    "# Get endpoint dates to store the file\n",
    "start, end = steps.date[0], steps.date[len(steps.index) - 1]\n",
    "\n",
    "# Drop date column and store\n",
    "steps.drop('date', axis=1, inplace=True)\n",
    "steps.to_csv(\"./data/tidy/steps_{}_to_{}.csv\".format(start, end), index=False)\n",
    "\n",
    "# Look at end result\n",
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Activity statistics per week day\n",
    "\n",
    "Let's compile some statistics based on day of the week. First, let's take a look at summary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use parse_dates to interpret our date column as datetime objects\n",
    "summary = pd.read_csv(\"./data/tidy/summary_2019-10-01_to_2020-05-18.csv\", parse_dates=['date'])\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use strftime to convert the date to a week day, and get group statistics per day of the week.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a weekday column\n",
    "summary['weekday'] = summary.date.apply(lambda x: datetime.strftime(x, \"%A\"))\n",
    "\n",
    "# Get statistics per day of the week\n",
    "weekly_statistics = summary.groupby('weekday').describe()\n",
    "\n",
    "# Row indices are days of the week, put them in order\n",
    "row_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekly_statistics = weekly_statistics.loc[row_order, :]\n",
    "\n",
    "# Show results\n",
    "weekly_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean and first quartile for number of steps per weekday\n",
    "mean_steps = weekly_statistics.steps[['mean', '25%']]\n",
    "mean_steps.plot(kind='bar')\n",
    "\n",
    "plt.title('Weekly stepcount since October 1st, 2019')\n",
    "plt.ylabel('steps')\n",
    "plt.ylim([0, 18000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Visualising walks over the day\n",
    "Let's now look at the steps intraday data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in dataset\n",
    "steps = pd.read_csv(\"./data/tidy/steps_2019-10-01_to_2020-05-18.csv\", parse_dates=['time'])\n",
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise steps intraday data over a given day. We look at May 1st again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2020-05-01'\n",
    "\n",
    "# Restrict to logs for given date\n",
    "day_df = steps[steps.time.apply(lambda x: datetime.strftime(x, \"%Y-%m-%d\")) == date].copy()\n",
    "\n",
    "# Restrict to within waking hours\n",
    "start_of_day = pd.to_datetime('2020-05-01 07:00:00')\n",
    "end_of_day = pd.to_datetime('2020-05-01 23:00:00')\n",
    "day_df = day_df[(day_df.time >= start_of_day)&(day_df.time <= end_of_day)]\n",
    "\n",
    "# Convert time back to hr:min:sec format and set as index\n",
    "day_df.time = day_df.time.apply(lambda x: datetime.strftime(x, \"%H:%M:%S\"))\n",
    "day_df.set_index('time', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot steps during the day on May 1st."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot steps on May 1st\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "day_df.rolling(15).mean().plot(ax=ax)  # 15 min rolling avg to smooth out noise\n",
    "ax.set_title('Steps on May 1st, 2020')\n",
    "ax.set_xlabel('Time of Day')\n",
    "ax.set_ylabel('Steps per min')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can tell which period corresponds to exercise, and which results from general activity, but let's be more systematic about this. We can isolate the steps that result from walks alone and not from general activity. The activity dataset has a start_time and end_time for each activity (walk, run, ...) and we may use these to filter our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load activities dataset, parsing start_time and end_time columns as datetime objects\n",
    "time_col = ['start_time', 'end_time']\n",
    "activities = pd.read_csv(\"./data/tidy/activities_2019-10-01_to_2020-05-18.csv\", parse_dates=time_col)\n",
    "activities.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column named 'on_walk' to the steps dataset, with a True/False value. For this we cook up a helper function as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to filter the intraday steps data by activity type\n",
    "def is_during_activity(t, activity):\n",
    "    \"\"\" Takes a datetime object t and activity name\n",
    "        Returns True if during activity, else False \"\"\"\n",
    "    # Get the activities dataset for that day\n",
    "    date = datetime.strftime(t, \"%Y-%m-%d\")\n",
    "    df = activities[activities.date == date]\n",
    "    \n",
    "    # Subset to rows which represent activity\n",
    "    df = df[df.name == activity]\n",
    "    \n",
    "    # Check if t is within the bounds of the activity\n",
    "    for i in df.index:\n",
    "        if df.loc[i, 'start_time'] <= t <= df.loc[i, 'end_time']:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "# Add 'on_walk' column to steps dataframe\n",
    "steps['on_walk'] = steps.time.apply(is_during_activity, args=('Walk',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the stepcount during walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps[steps.on_walk == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this, we can create a new dataframe consisting of walks stepcount data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all steps outside of walks to zero\n",
    "walks = steps.copy()\n",
    "walks.stepcount = walks.stepcount.where(walks.on_walk == True, 0)\n",
    "    \n",
    "# Drop 'on_walk' column\n",
    "walks.drop('on_walk', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at May 1st again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2020-05-01'\n",
    "\n",
    "# Restrict to logs for given date\n",
    "day_walks = walks[walks.time.apply(lambda x: datetime.strftime(x, \"%Y-%m-%d\")) == date].copy()\n",
    "\n",
    "# Restrict to within waking hours\n",
    "start_of_day = pd.to_datetime('2020-05-01 07:00:00')\n",
    "end_of_day = pd.to_datetime('2020-05-01 23:00:00')\n",
    "day_walks = day_walks[(day_walks.time >= start_of_day)&(day_walks.time <= end_of_day)]\n",
    "\n",
    "# Convert time back to hr:min:sec format and set as index\n",
    "day_walks.time = day_walks.time.apply(lambda x: datetime.strftime(x, \"%H:%M:%S\"))\n",
    "day_walks.set_index('time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot walks on May 1st\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "day_walks.rolling(15).mean().plot(ax=ax)  # 15 min rolling avg to smooth out noise\n",
    "ax.set_title('Steps on May 1st 2020 during a walk')\n",
    "ax.set_xlabel('Time of Day')\n",
    "ax.set_ylabel('Steps per min')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise walk times for each day of the week.\n",
    "\n",
    "We can build a picture of the 'average' day over the last 5 months, broken down by day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a weekday column to walks dataset for grouping\n",
    "walks['weekday'] = walks.time.apply(lambda x: datetime.strftime(x, \"%A\"))\n",
    "walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our daily picture, let's first group the dataset by day of the week, then average the stepcount for each given minute. This should give us a sense of the distribution of walks on each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change date column to hour:min strings for grouping\n",
    "walks.time = walks.time.apply(lambda x: datetime.strftime(x, \"%H:%M\"))\n",
    "\n",
    "# for each day of the week, average step count over all dates\n",
    "walks_weekday = walks.groupby('weekday') \n",
    "\n",
    "weekdays = {}\n",
    "for day_name, df in walks_weekday:\n",
    "    # group by minute, then average over dates\n",
    "    df = df.groupby('time').mean()\n",
    "    weekdays[day_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get rid of the timestamps during the night, since I'm not up for midnight walks too often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to waking hours, say 7:00am to 23:59pm\n",
    "for day in weekdays:\n",
    "    weekdays[day] = weekdays[day].iloc[420:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the distribution of walks on Mondays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays['Monday'].rolling(15).mean().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we do this for each day of the week separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each day of the week\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "fig1, axes1 = plt.subplots(1, 5, figsize=(25, 5))\n",
    "fig2, axes2 = plt.subplots(1, 2, figsize=(25, 5))\n",
    "\n",
    "# Plot Monday-Friday first\n",
    "for i in range(5):\n",
    "    # Take 15min rolling average\n",
    "    df = weekdays[days[i]].rolling(15).mean()\n",
    "    \n",
    "    # Relabel\n",
    "    df.rename(columns={'stepcount': 'steps/min'}, inplace=True)\n",
    "    \n",
    "    # Plot day\n",
    "    df.plot(ax=axes1[i])\n",
    "    axes1[i].set_title(days[i])\n",
    "    axes1[i].set_xlabel(\"Time of Day\")\n",
    "    \n",
    "# Then plot Saturday-Sunday\n",
    "for i in range(2):\n",
    "    # Take 15 min rolling average\n",
    "    df = weekdays[days[5+i]].rolling(15).mean()\n",
    "    \n",
    "    # Relabel\n",
    "    df.rename(columns={'stepcount': 'steps/min'}, inplace=True)\n",
    "\n",
    "    # Plot day\n",
    "    df.plot(ax=axes2[i])\n",
    "    axes2[i].set_title(days[5+i])\n",
    "    axes2[i].set_xlabel(\"Time of Day\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fitbit data virtualenv",
   "language": "python",
   "name": "fitbit_data_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
